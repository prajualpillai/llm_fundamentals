{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Split data into useful words/characters -> tokens\n",
    "split_data = re.split('([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "split_data = [data for data in split_data if data.strip()]\n",
    "\n",
    "# Convert tokens to token_ids\n",
    "\"\"\"\n",
    "1. Build vocabulary: In alphabeltical order\n",
    "2. Each unique token is assigned to a unique id\n",
    "\"\"\"\n",
    "all_words = sorted(set(split_data))\n",
    "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab_size = len(all_words)\n",
    "vocab = {}\n",
    "for i in range(vocab_size):\n",
    "    vocab[all_words[i]] = i            # Can be seen as a very simple encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class CustomTokernizerV1:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def read_data(self, path):\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        return raw_text\n",
    "    \n",
    "    def encode(self, raw_text):\n",
    "\n",
    "        split_data = re.split('([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "        split_data = [data for data in split_data if data.strip()]\n",
    "        ids = [self.str_to_int[s] for s in split_data]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93, 933, 584, 904]\n",
      "The strain is small\n"
     ]
    }
   ],
   "source": [
    "obj = CustomTokernizerV1(vocab=vocab)\n",
    "print(obj.encode(\"The strain is small\"))\n",
    "print(obj.decode([93, 933, 584, 904]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding special context tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class CustomTokernizerV2:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def read_data(self, path):\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        \n",
    "        return raw_text\n",
    "    \n",
    "    def encode(self, raw_text):\n",
    "\n",
    "        split_data = re.split('([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "        split_data = [data for data in split_data if data.strip()]\n",
    "        \n",
    "        ids = [self.str_to_int[s] if s in self.str_to_int \n",
    "               else self.str_to_int[\"<|unk|>\"] \n",
    "               for s in split_data]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1131, 5, 93, 933, 584, 904, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, The strain is small <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "obj = CustomTokernizerV2(vocab=vocab)\n",
    "\n",
    "text1 = \"hello, The strain is small\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join([text1, text2])\n",
    "ids = obj.encode(text)\n",
    "print(ids)\n",
    "print(obj.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other special tokens:\n",
    "1. BOS: Beginning of sequence\n",
    "2. EOS: End of sequence\n",
    "3. PAD: To ensure all texts of different sizes are of same size, padding is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
